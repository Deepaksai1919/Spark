{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/19 15:39:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('json-rdd').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bc0140581257:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>json-rdd</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=json-rdd>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rdd_json = sc.textFile('file:/config/workspace/JsonAnalysis/kaggle/arxiv/dataset.json/arxiv/arxiv-metadata-oai-snapshot.json', 100)\n",
    "rdd = rdd_json.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '0704.0001',\n",
       "  'submitter': 'Pavel Nadolsky',\n",
       "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       "  'comments': '37 pages, 15 figures; published version',\n",
       "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       "  'doi': '10.1103/PhysRevD.76.013009',\n",
       "  'report-no': 'ANL-HEP-PR-07-12',\n",
       "  'categories': 'hep-ph',\n",
       "  'license': None,\n",
       "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       "  'update_date': '2008-11-26',\n",
       "  'authors_parsed': [['Balázs', 'C.', ''],\n",
       "   ['Berger', 'E. L.', ''],\n",
       "   ['Nadolsky', 'P. M.', ''],\n",
       "   ['Yuan', 'C. -P.', '']]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2276611"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get count\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['authors_parsed',\n",
       " 'license',\n",
       " 'authors',\n",
       " 'versions',\n",
       " 'id',\n",
       " 'doi',\n",
       " 'abstract',\n",
       " 'journal-ref',\n",
       " 'update_date',\n",
       " 'title',\n",
       " 'submitter',\n",
       " 'categories',\n",
       " 'report-no',\n",
       " 'comments']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all attributes\n",
    "\n",
    "rdd.flatMap(lambda x:x.keys()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n",
       " 'http://creativecommons.org/licenses/by-nc-nd/4.0/',\n",
       " 'http://creativecommons.org/licenses/by-nc-sa/4.0/',\n",
       " 'http://creativecommons.org/publicdomain/zero/1.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/4.0/',\n",
       " 'http://creativecommons.org/licenses/publicdomain/',\n",
       " 'http://creativecommons.org/licenses/by/3.0/',\n",
       " 'http://creativecommons.org/licenses/by/4.0/',\n",
       " 'http://creativecommons.org/licenses/by-nc-sa/3.0/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get distinct licenses\n",
    "rdd.map(lambda x: x['license']).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>    (103 + 10) / 113]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Title: 0\n",
      "Largest Title: Investigation of the 2-body system with a rotating central body (e. g.\n",
      "  earth-moon system) within the Projective Unified Field theory: the transfer\n",
      "  of rotational angular momentum and energy from the central body to the\n",
      "  orbital 2-body system, the tidal and the non-tidal influences (mechanical,\n",
      "  general-relativistic Lense-Thirring effect and cosmological\n",
      "  PUFT-contributions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get shortest and longest titles\n",
    "titles_rdd = rdd.map(lambda x: x['title'])\n",
    "\n",
    "shortest_title = titles_rdd.reduce(lambda x,y: x if len(x) < len(y) else y )\n",
    "largest_title = titles_rdd.reduce(lambda x,y: x if len(x) > len(y) else y )\n",
    "print('Shortest Title:', shortest_title)\n",
    "print('Largest Title:', largest_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find abbreviations with 5 or more letters in the abstract\n",
    "import re\n",
    "\n",
    "def get_abbreviations(line):\n",
    "    result = re.search(r\"\\(([A-Za-z][^_/\\\\<>]{5,})\\)\", line)\n",
    "    if result:\n",
    "        return result.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "816661"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: get_abbreviations(x['abstract'])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0704.0001',\n",
       "  'submitter': 'Pavel Nadolsky',\n",
       "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
       "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       "  'comments': '37 pages, 15 figures; published version',\n",
       "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       "  'doi': '10.1103/PhysRevD.76.013009',\n",
       "  'report-no': 'ANL-HEP-PR-07-12',\n",
       "  'categories': 'hep-ph',\n",
       "  'license': None,\n",
       "  'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
       "  'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'},\n",
       "   {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}],\n",
       "  'update_date': '2008-11-26',\n",
       "  'authors_parsed': [['Balázs', 'C.', ''],\n",
       "   ['Berger', 'E. L.', ''],\n",
       "   ['Nadolsky', 'P. M.', ''],\n",
       "   ['Yuan', 'C. -P.', '']]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of archive records per month (update_date attribute)\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_month(date_string):\n",
    "    date = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "    return date.month\n",
    "\n",
    "rdd_month = rdd.map(lambda x: (extract_month(x['update_date']), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 1),\n",
       " (12, 1),\n",
       " (1, 1),\n",
       " (5, 1),\n",
       " (10, 1),\n",
       " (5, 1),\n",
       " (11, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (5, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_month.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7, 134445),\n",
       " (12, 139647),\n",
       " (1, 142071),\n",
       " (9, 150865),\n",
       " (8, 151084),\n",
       " (2, 151162),\n",
       " (4, 153724),\n",
       " (3, 170341),\n",
       " (10, 209438),\n",
       " (6, 222547),\n",
       " (11, 309797),\n",
       " (5, 341490)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_month.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>(112 + 1) / 113]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of pages: 18.06259141637006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35114)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/conda/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/conda/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/conda/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/conda/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of pages (take from comments)\n",
    "import re\n",
    "def get_page(comment):\n",
    "    if not comment:\n",
    "        return 0\n",
    "    search = re.findall('\\d+ pages', comment)\n",
    "    if search:\n",
    "        return int(search[0].split()[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "rdd_average = rdd.map(lambda x: get_page(x['comments'])).filter(lambda x: x!= 0)\n",
    "\n",
    "average_count = rdd_average.count()\n",
    "average_summation = rdd_average.reduce(lambda x,y: x+y)\n",
    "\n",
    "print('Average number of pages:', average_summation / average_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
