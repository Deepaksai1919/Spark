{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd77f7b5-28d4-4595-9ac3-0ff186d8af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a457223c-b8bf-4419-8754-8e36d23b7aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 22:36:25 WARN Utils: Your hostname, OMEN resolves to a loopback address: 127.0.1.1; using 172.19.181.52 instead (on interface eth0)\n",
      "23/06/30 22:36:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/30 22:36:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.19.181.52:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DF DataSkew Optimization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f421009ce20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[4]').appName('DF DataSkew Optimization').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7944037-3048-49b5-8ee5-d3a37d3ca7e9",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "### Sale dataset:\n",
    "#### **Table1**: OrderId, Qty, Sales, Discount (yes=1, no=0)\n",
    "#### **Table2**: ProductId, OrderId, Product, Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21d36ec-41cd-4361-b480-5343b356d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb739e4-12b3-4aa5-ae14-e7ac18e94f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1:\n",
    "# (key, key_count, quantity, sales)\n",
    "config = (101,100,100,(10,100)),(201,7000000,200,(50,3400)),(301,500,1000,(12,2000)),(401,10000,50,(40,1000))\n",
    "OrderID = []\n",
    "Sales = []\n",
    "Qty = []\n",
    "\n",
    "for key, key_count, quantity, sales in config:\n",
    "    OrderID.extend([key] * key_count)\n",
    "    Qty.extend(list(np.random.randint(low=1, high=quantity, size=key_count)))\n",
    "    Sales.extend(list(np.random.randint(low=sales[0], high=sales[1], size=key_count)))\n",
    "random.shuffle(OrderID)\n",
    "Discount = list(np.random.randint(low=0, high=2, size=len(OrderID)))\n",
    "data1 = list(zip(OrderID, Qty, Sales, Discount))\n",
    "\n",
    "data_skew = pd.DataFrame(data1, columns=['OrderID', 'Qty', 'Sales', 'Discount'])\n",
    "\n",
    "# Table 2:\n",
    "data2 = [\n",
    "    [1, 101, 'Pencil', 4.99],\n",
    "    [2, 101, 'book', 9.5],\n",
    "    [3, 101, 'scissors', 14],\n",
    "    [4, 301, 'glue', 7],\n",
    "    [5, 201, 'marker', 8.49],\n",
    "    [6, 301, 'label', 2],\n",
    "    [7, 201, 'calculator', 3.99],\n",
    "    [8, 501, 'eraser', 1.55]\n",
    "]\n",
    "\n",
    "data_small = pd.DataFrame(data2, columns=['ProductID', 'OrderID', 'Product', 'Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8dc9752-5cbd-42fb-a399-6a8862797a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+--------+\n",
      "|OrderID|Qty|Sales|Discount|\n",
      "+-------+---+-----+--------+\n",
      "|    201| 70|   75|       0|\n",
      "|    201| 49|   28|       0|\n",
      "|    201| 32|   79|       1|\n",
      "|    201| 42|   69|       1|\n",
      "|    201| 44|   39|       0|\n",
      "|    201| 51|   56|       1|\n",
      "|    201| 15|   47|       0|\n",
      "|    201| 95|   35|       1|\n",
      "|    201| 20|   73|       1|\n",
      "|    201|  5|   13|       0|\n",
      "|    201| 83|   16|       1|\n",
      "|    201| 65|   72|       1|\n",
      "|    201| 90|   46|       0|\n",
      "|    201|  9|   88|       0|\n",
      "|    201| 12|   18|       0|\n",
      "|    201| 85|   95|       0|\n",
      "|    201| 61|   91|       1|\n",
      "|    201| 69|   38|       1|\n",
      "|    201| 51|   30|       0|\n",
      "|    201| 43|   87|       1|\n",
      "+-------+---+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 22:46:58 WARN TaskSetManager: Stage 8 contains a task of very large size (54752 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize conversion between PySpark and Pandas dataframes\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "\n",
    "df_skew = spark.createDataFrame(data_skew)\n",
    "df_skew = df_skew.coalesce(4)\n",
    "df_skew.show()\n",
    "df_skew.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b19e515f-162d-44f0-90ee-7e6d52a02794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- OrderID: long (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n",
      "+---------+-------+----------+-----+\n",
      "|ProductID|OrderID|   Product|Price|\n",
      "+---------+-------+----------+-----+\n",
      "|        1|    101|    Pencil| 4.99|\n",
      "|        2|    101|      book|  9.5|\n",
      "|        3|    101|  scissors| 14.0|\n",
      "|        4|    301|      glue|  7.0|\n",
      "|        5|    201|    marker| 8.49|\n",
      "|        6|    301|     label|  2.0|\n",
      "|        7|    201|calculator| 3.99|\n",
      "|        8|    501|    eraser| 1.55|\n",
      "+---------+-------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = spark.createDataFrame(data_small)\n",
    "df_small.printSchema()\n",
    "df_small.show()\n",
    "df_small.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5116870c-5941-440a-abd8-f890f450c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_skew.join(df_small, on='OrderID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5368d16-cb0c-4966-a0fb-850da4e5ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+--------+---------+----------+-----+\n",
      "|OrderID|Qty|Sales|Discount|ProductID|   Product|Price|\n",
      "+-------+---+-----+--------+---------+----------+-----+\n",
      "|    201| 70|   75|       0|        7|calculator| 3.99|\n",
      "|    201| 70|   75|       0|        5|    marker| 8.49|\n",
      "|    201| 49|   28|       0|        7|calculator| 3.99|\n",
      "|    201| 49|   28|       0|        5|    marker| 8.49|\n",
      "|    201| 32|   79|       1|        7|calculator| 3.99|\n",
      "|    201| 32|   79|       1|        5|    marker| 8.49|\n",
      "|    201| 42|   69|       1|        7|calculator| 3.99|\n",
      "|    201| 42|   69|       1|        5|    marker| 8.49|\n",
      "|    201| 44|   39|       0|        7|calculator| 3.99|\n",
      "|    201| 44|   39|       0|        5|    marker| 8.49|\n",
      "|    201| 51|   56|       1|        7|calculator| 3.99|\n",
      "|    201| 51|   56|       1|        5|    marker| 8.49|\n",
      "|    201| 15|   47|       0|        7|calculator| 3.99|\n",
      "|    201| 15|   47|       0|        5|    marker| 8.49|\n",
      "|    201| 95|   35|       1|        7|calculator| 3.99|\n",
      "|    201| 95|   35|       1|        5|    marker| 8.49|\n",
      "|    201| 20|   73|       1|        7|calculator| 3.99|\n",
      "|    201| 20|   73|       1|        5|    marker| 8.49|\n",
      "|    201|  5|   13|       0|        7|calculator| 3.99|\n",
      "|    201|  5|   13|       0|        5|    marker| 8.49|\n",
      "|    201| 83|   16|       1|        7|calculator| 3.99|\n",
      "|    201| 83|   16|       1|        5|    marker| 8.49|\n",
      "|    201| 65|   72|       1|        7|calculator| 3.99|\n",
      "|    201| 65|   72|       1|        5|    marker| 8.49|\n",
      "|    201| 90|   46|       0|        7|calculator| 3.99|\n",
      "|    201| 90|   46|       0|        5|    marker| 8.49|\n",
      "|    201|  9|   88|       0|        7|calculator| 3.99|\n",
      "|    201|  9|   88|       0|        5|    marker| 8.49|\n",
      "|    201| 12|   18|       0|        7|calculator| 3.99|\n",
      "|    201| 12|   18|       0|        5|    marker| 8.49|\n",
      "+-------+---+-----+--------+---------+----------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 22:48:34 WARN TaskSetManager: Stage 10 contains a task of very large size (54752 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1071e0-8e6b-4bfb-8b2a-31702bd362f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 22:48:39 WARN TaskSetManager: Stage 12 contains a task of very large size (54752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14001300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd595b2-3d17-4411-a33d-5a49d5c8bb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2e6088d-e016-40cc-a3f3-978948f94265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff1ac350-6fd8-4faa-a6e8-13fc793cfe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdfe3170-51b2-42d4-a689-2c693ab3f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b31d723-ad59-4b33-a7e9-cbe808e00359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 22:52:19 WARN TaskSetManager: Stage 17 contains a task of very large size (54752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 17:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+----------+----------+\n",
      "|        AVG(Sales)|       STD(Sales)|MIN(Sales)|MAX(Sales)|\n",
      "+------------------+-----------------+----------+----------+\n",
      "|1722.4612253147923|967.3834016356338|        11|      3399|\n",
      "+------------------+-----------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary = joined_df.select(\n",
    "    mean(joined_df.Sales).alias('AVG(Sales)'),\n",
    "    stddev(joined_df.Sales).alias('STD(Sales)'),\n",
    "    min(joined_df.Sales).alias('MIN(Sales)'),\n",
    "    max(joined_df.Sales).alias('MAX(Sales)')\n",
    ")\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7314138-8766-4071-a00d-caf078b8554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+--------+------+\n",
      "|OrderID|Qty|Sales|Discount|_salt_|\n",
      "+-------+---+-----+--------+------+\n",
      "|    201| 70|   75|       0|   0.0|\n",
      "|    201| 49|   28|       0|   1.0|\n",
      "|    201| 32|   79|       1|   1.0|\n",
      "|    201| 42|   69|       1|   0.0|\n",
      "|    201| 44|   39|       0|   1.0|\n",
      "|    201| 51|   56|       1|   2.0|\n",
      "|    201| 15|   47|       0|   2.0|\n",
      "|    201| 95|   35|       1|   1.0|\n",
      "|    201| 20|   73|       1|   2.0|\n",
      "|    201|  5|   13|       0|   1.0|\n",
      "|    201| 83|   16|       1|   0.0|\n",
      "|    201| 65|   72|       1|   0.0|\n",
      "|    201| 90|   46|       0|   1.0|\n",
      "|    201|  9|   88|       0|   1.0|\n",
      "|    201| 12|   18|       0|   0.0|\n",
      "|    201| 85|   95|       0|   0.0|\n",
      "|    201| 61|   91|       1|   0.0|\n",
      "|    201| 69|   38|       1|   0.0|\n",
      "|    201| 51|   30|       0|   1.0|\n",
      "|    201| 43|   87|       1|   1.0|\n",
      "+-------+---+-----+--------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+----------+-----+------+\n",
      "|ProductID|OrderID|   Product|Price|_salt_|\n",
      "+---------+-------+----------+-----+------+\n",
      "|        1|    101|    Pencil| 4.99|   2.0|\n",
      "|        2|    101|      book|  9.5|   2.0|\n",
      "|        3|    101|  scissors| 14.0|   1.0|\n",
      "|        4|    301|      glue|  7.0|   1.0|\n",
      "|        5|    201|    marker| 8.49|   1.0|\n",
      "|        6|    301|     label|  2.0|   2.0|\n",
      "|        7|    201|calculator| 3.99|   0.0|\n",
      "|        8|    501|    eraser| 1.55|   2.0|\n",
      "+---------+-------+----------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 23:05:04 WARN TaskSetManager: Stage 20 contains a task of very large size (54752 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "df_skew_salting = df_skew.withColumn('_salt_', round(rand() * 2))\n",
    "df_small_salting = df_small.withColumn('_salt_', round(rand() * 2))\n",
    "\n",
    "df_skew_salting.show()\n",
    "df_small_salting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f622748-65e4-416a-8a01-ed5a0257b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition using salting\n",
    "df_skew_salting = df_skew_salting.repartition(8, '_salt_')\n",
    "df_small_salting = df_small_salting.repartition(8, '_salt_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17a85302-7edd-434a-aa7d-0aefe6988846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skew_salting = df_skew_salting.drop('_salt_')\n",
    "df_small_salting = df_small_salting.drop('_salt_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da1b198-c8a4-461a-8480-ad2f4c7c1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df_salting = df_skew_salting.join(df_small_salting, on='OrderID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2519e68-7f0e-4ee2-b0ea-2281cf01d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 23:09:02 WARN TaskSetManager: Stage 21 contains a task of very large size (54752 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 26:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+----------+----------+\n",
      "|        AVG(Sales)|       STD(Sales)|MIN(Sales)|MAX(Sales)|\n",
      "+------------------+-----------------+----------+----------+\n",
      "|1722.4612253147923|967.3834016355883|        11|      3399|\n",
      "+------------------+-----------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_salting = joined_df_salting.select(\n",
    "    mean(joined_df.Sales).alias('AVG(Sales)'),\n",
    "    stddev(joined_df.Sales).alias('STD(Sales)'),\n",
    "    min(joined_df.Sales).alias('MIN(Sales)'),\n",
    "    max(joined_df.Sales).alias('MAX(Sales)')\n",
    ")\n",
    "summary_salting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ad90b-8def-4ff8-a3f6-2f066ffac905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
